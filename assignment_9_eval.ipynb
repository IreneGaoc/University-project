{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_9_eval.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Wb3btmXM6Tqv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.draw import polygon\n",
        "\n",
        "\n",
        "def compute_classification_acc(pred, gt):\n",
        "  # pred and gt are both\n",
        "  assert pred.shape == gt.shape\n",
        "  return (pred == gt).astype(int).sum() / gt.size\n",
        "    \n",
        "    \n",
        "def compute_iou(b_pred,b_gt):\n",
        "  # b_pred: predicted bounding boxes, shape=(n,2,4)\n",
        "  # b_gt: ground truth bounding boxes, shape=(n,2,4)\n",
        "    \n",
        "  n = np.shape(b_gt)[0]\n",
        "  L_pred = np.zeros((64,64))\n",
        "  L_gt = np.zeros((64,64))\n",
        "  iou = 0.0\n",
        "  for i in range(n):\n",
        "    for b in range(2):\n",
        "      rr, cc = polygon([b_pred[i,b,0],b_pred[i,b,0],b_pred[i,b,2],b_pred[i,b,2]],\n",
        "                   [b_pred[i,b,1],b_pred[i,b,3],b_pred[i,b,3],b_pred[i,b,1]],[64,64])\n",
        "      L_pred[rr,cc] = 1\n",
        "\n",
        "      rr, cc = polygon([b_gt[i,b,0],b_gt[i,b,0],b_gt[i,b,2],b_gt[i,b,2]],\n",
        "                      [b_gt[i,b,1],b_gt[i,b,3],b_gt[i,b,3],b_gt[i,b,1]],[64,64])\n",
        "      L_gt[rr,cc] = 1\n",
        "\n",
        "      iou += (1.0/(2*n))*(np.sum((L_pred+L_gt)==2)/np.sum((L_pred+L_gt)>=1))\n",
        "\n",
        "      L_pred[:,:] = 0\n",
        "      L_gt[:,:] = 0\n",
        "    \n",
        "  return iou\n",
        "\n",
        "\n",
        "def evaluation(pred_class,pred_bboxes,prefix=\"valid\"):\n",
        "  #pred_class = mnist_classification(x,y)   #shape[N,2]\n",
        "  # pred_bboxes: Your predicted bboxes for 2 digits, shape [N, 2, 4]\n",
        "  gt_class = np.load(prefix + \"_Y.npy\")\n",
        "  gt_bboxes = np.load(prefix + \"_bboxes.npy\")\n",
        "  acc = compute_classification_acc(pred_class, gt_class)\n",
        "  iou = compute_iou(pred_bboxes, gt_bboxes)\n",
        "  print(f\"Classification Acc: {acc}\")\n",
        "  print(f\"BBoxes IOU: {iou}\")\n",
        "  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7MGk4uIWIAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "class MNIST(object):\n",
        "    def __init__(self, subset='train', batch_size=100, shuffle=True):\n",
        "        if subset == 'train':\n",
        "            \n",
        "            images = np.load('train_X.npy')\n",
        "            images = np.reshape(images,(np.shape(images)[0], 64, 64, 1))\n",
        "            labels = np.load('train_Y.npy')\n",
        "            bboxes = np.load('train_bboxes.npy')\n",
        "            bboxes = bboxes[:,:,:]\n",
        "            print(\"training set:\", images.shape, labels.shape,bboxes.shape)\n",
        "        elif subset == 'valid':\n",
        "            images = np.load('valid_X.npy')\n",
        "            images = np.reshape(images, (np.shape(images)[0], 64, 64, 1))\n",
        "            labels = np.load('valid_Y.npy')\n",
        "            bboxes = np.load('valid_bboxes.npy')\n",
        "            bboxes = bboxes[:,:,:] \n",
        "            print(\"validation set:\", images.shape, labels.shape,bboxes.shape)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        \n",
        "        self._images = images\n",
        "        self.images = self._images\n",
        "        self._labels = labels\n",
        "        self.labels = self._labels\n",
        "        self._bboxes = bboxes\n",
        "        self.bboxes = self._bboxes\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.num_samples = len(self.images)\n",
        "        self.shuffle = shuffle\n",
        "        if self.shuffle:\n",
        "            self.shuffle_samples()\n",
        "        self.next_batch_pointer = 0\n",
        "    def shuffle_samples(self):\n",
        "        image_indices = np.random.permutation(np.arange(self.num_samples))\n",
        "        self.images = self._images[image_indices]\n",
        "        self.labels = self._labels[image_indices]\n",
        "        self.bboxes = self._bboxes[image_indices]\n",
        "    def get_next_batch(self):\n",
        "        num_samples_left = self.num_samples - self.next_batch_pointer\n",
        "        if num_samples_left >= self.batch_size:\n",
        "            x_batch = self.images[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n",
        "            y_batch = self.labels[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n",
        "            b_batch = self.bboxes[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n",
        "            self.next_batch_pointer += self.batch_size\n",
        "        else:\n",
        "            x_partial_batch_1 = self.images[self.next_batch_pointer:self.num_samples]\n",
        "            y_partial_batch_1 = self.labels[self.next_batch_pointer:self.num_samples]\n",
        "            b_partial_batch_1 = self.bboxes[self.next_batch_pointer:self.num_samples]\n",
        "            if self.shuffle:\n",
        "                self.shuffle_samples()\n",
        "            x_partial_batch_2 = self.images[0:self.batch_size - num_samples_left]\n",
        "            y_partial_batch_2 = self.labels[0:self.batch_size - num_samples_left]\n",
        "            b_partial_batch_2 = self.bboxes[0:self.batch_size - num_samples_left]\n",
        "            x_batch = np.vstack((x_partial_batch_1, x_partial_batch_2))\n",
        "            y_batch = np.vstack((y_partial_batch_1, y_partial_batch_2))\n",
        "            b_batch = np.vstack((b_partial_batch_1, b_partial_batch_2))\n",
        "            \n",
        "            self.next_batch_pointer = self.batch_size - num_samples_left\n",
        "\n",
        "        return x_batch, y_batch,b_batch\n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KFnULDz--yPm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def net(input, is_training,keep_prob):\n",
        "\n",
        "\n",
        "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = 0, stddev = 0.1), name='conv1_W')\n",
        "    conv1_b = tf.Variable(tf.zeros(6), name='conv1_b')\n",
        "    conv1   = tf.nn.conv2d(input, conv1_W, strides=[1, 1, 1, 1], padding='SAME') + conv1_b\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "\n",
        "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = 0, stddev = 0.1), name='conv2_W')\n",
        "    conv2_b = tf.Variable(tf.zeros(16), name='conv2_b')\n",
        "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "\n",
        "    conv3_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 32), mean = 0, stddev = 0.1), name='conv3_W')\n",
        "    conv3_b = tf.Variable(tf.zeros(32), name='conv3_b')\n",
        "    conv3  = tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='SAME') + conv3_b\n",
        "    conv3 = tf.nn.relu(conv3)\n",
        "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "\n",
        "    fc0   = flatten(conv3)\n",
        "    fc0 = tf.layers.dropout(fc0, rate=keep_prob, training=is_training)\n",
        "\n",
        "    fc1_W = tf.Variable(tf.truncated_normal(shape=(1568, 1300), mean = 0, stddev = 0.1), name='fc1_W')\n",
        "    fc1_b = tf.Variable(tf.zeros(1300), name='fc1_b')\n",
        "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
        "    fc1    = tf.nn.relu(fc1)\n",
        "\n",
        "\n",
        "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(1300, 840), mean = 0, stddev = 0.1), name='fc2_W')\n",
        "    fc2_b  = tf.Variable(tf.zeros(840), name='fc2_b')\n",
        "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
        "    fc2    = tf.nn.relu(fc2)\n",
        "    \n",
        "    return fc2\n",
        "\n",
        "def find_class_net(fc2):\n",
        "\n",
        "    W1 = tf.get_variable('W1', [840, 10*2], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "    b1 = tf.get_variable('b1', [10*2], initializer=tf.constant_initializer(0.1))\n",
        "    fc_out1 = tf.matmul(fc2, W1) + b1\n",
        "    fc_logits = tf.reshape(fc_out1, [-1, 2, 10])\n",
        "    \n",
        "    return fc_logits\n",
        "\n",
        "\n",
        "def find_b_net(fc2):\n",
        "    W2 = tf.get_variable('W2', [840, 4*2], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "    b2 = tf.get_variable('b2', [4*2], initializer=tf.constant_initializer(0.1))\n",
        "    fc_out2 = tf.matmul(fc2, W2) + b2\n",
        "    fc_bbox = tf.reshape(fc_out2, [-1, 2, 4])\n",
        "\n",
        "    return fc_bbox\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OGHBHOV1UYMD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def train():\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    BATCH_SIZE = 80\n",
        "    NUM_ITERS = 50000\n",
        "\n",
        "    train_set = MNIST('train', batch_size=BATCH_SIZE)\n",
        "    valid_set = MNIST('valid')\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    x = tf.placeholder(tf.float32, (None, 64, 64,1))\n",
        "\n",
        "    y = tf.placeholder(tf.int32, (None, 2))\n",
        "    y = tf.reshape(y,(-1,2))\n",
        "\n",
        "    b = tf.placeholder(tf.float32, (None,2,4))\n",
        "    b = tf.reshape(b,(-1,2,4))\n",
        "    is_training = tf.placeholder(tf.bool, ())\n",
        "\n",
        "\n",
        "    rate = 0.0008\n",
        "    fc_y = net(x, True,0.5)\n",
        "    fc_b = net(x, True,0.2)\n",
        "\n",
        "    fc_logits =  find_class_net(fc_y)\n",
        "    bboxes =  find_b_net(fc_b)\n",
        "\n",
        "    prediction = tf.argmax(fc_logits, dimension=2)\n",
        "    cross_entropy1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=fc_logits[:,0], labels=y[:,0]))\n",
        "    cross_entropy2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=fc_logits[:,1], labels=y[:,1]))\n",
        "    cross_entropy = cross_entropy1 + cross_entropy2\n",
        "\n",
        "    loss_bboxes = tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(b, bboxes)))))\n",
        "    loss_operation = cross_entropy  +loss_bboxes\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=rate).minimize(loss_operation)\n",
        "\n",
        "    pred_y = prediction\n",
        "    pred_b = bboxes\n",
        "    saver = tf.train.Saver(max_to_keep=0)\n",
        "\n",
        "    print(\"Training for classification and detection...\")\n",
        "    print(\"let's start!!! \")\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        for i in range(NUM_ITERS):\n",
        "            batch_x, batch_y,batch_b = train_set.get_next_batch()\n",
        "            _ = sess.run(optimizer, feed_dict = {x:batch_x, y:batch_y,b:batch_b})\n",
        "\n",
        "            predd1,predd2= sess.run([pred_y,pred_b],feed_dict = {x:batch_x, y:batch_y,b:batch_b})\n",
        "            \n",
        "            predd1 = sess.run(pred_y,feed_dict = {x:batch_x, y:batch_y,b:batch_b})\n",
        "\n",
        "            predd2 = sess.run(pred_b,feed_dict = {x:batch_x, y:batch_y,b:batch_b})\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                acc1 = compute_classification_acc(predd1, batch_y)\n",
        "                acc2 = compute_iou(predd2,batch_b)\n",
        "\n",
        "                print(\"item: \", i, \"current classification accuracy:\",acc1,\"current iou accuracy:\",acc2)\n",
        "\n",
        "        saver.save(sess, 'ckpt/lenet', global_step = i)\n",
        "\n",
        "        print(\"Model saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLZfyCILVHEH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(imgg):\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    x = tf.placeholder(tf.float32, (None, 64, 64, 1))\n",
        "    fc_y = net(x, False,1)\n",
        "    fc_b = net(x, False,1)\n",
        "\n",
        "    fc_logits =  find_class_net(fc_y)\n",
        "    bboxes =  find_b_net(fc_b)\n",
        "\n",
        "    prediction = tf.argmax(fc_logits, dimension=2)\n",
        "    pred_y = prediction\n",
        "    pred_b = bboxes\n",
        "    saver = tf.train.Saver(max_to_keep=0)\n",
        "    valid_set = MNIST('valid',shuffle = False)\n",
        "    imgg = valid_set._images\n",
        "    \n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Testing...\")\n",
        "\n",
        "        saver.restore(sess, tf.train.latest_checkpoint('ckpt'))\n",
        "        yp_test,yb_test = sess.run([pred_y,pred_b], feed_dict = {x:imgg})\n",
        "        yp_test = sess.run(pred_y, feed_dict = {x:imgg})\n",
        "        yb_test = sess.run(pred_b, feed_dict = {x:imgg})\n",
        "      \n",
        "    return yp_test,yb_test\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ar9ZT_u8_qTy",
        "colab_type": "code",
        "outputId": "1d0e425b-678f-4916-ce77-6c63794d464b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    TRAIN = False\n",
        "    if TRAIN:\n",
        "        train()       \n",
        "    valid_set = MNIST('valid',shuffle = False)\n",
        "\n",
        "    imgg = valid_set._images       \n",
        "\n",
        "    predicted_y,predicted_b = test(imgg)    \n",
        "    \n",
        "\n",
        "    acc = evaluation(predicted_y,predicted_b, prefix=\"valid\")     #classification and detection\n",
        "    print(\"Done! Have a nice day : ) \")\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation set: (5000, 64, 64, 1) (5000, 2) (5000, 2, 4)\n",
            "validation set: (5000, 64, 64, 1) (5000, 2) (5000, 2, 4)\n",
            "Testing...\n",
            "INFO:tensorflow:Restoring parameters from ckpt/lenet-49999\n",
            "Classification Acc: 0.9576\n",
            "BBoxes IOU: 0.8054558046382353\n",
            "Done! Have a nice day : ) \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}