{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Term Customer Learning\n",
    "For customers with greater or equal to 6 months records, apply LSTM sequence classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    " For now, only testset(2000 customers) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading Done\n",
      "        Unnamed: 0           CID          DATE   PREV_STATUS      N_CR_ABM  \\\n",
      "count  61579.00000  6.157900e+04  6.157900e+04  61579.000000  61579.000000   \n",
      "mean   30789.00000  1.503712e+06  1.507144e+09      0.035207      0.193085   \n",
      "std    17776.47045  6.052459e+05  2.600310e+07      0.184304      0.920835   \n",
      "min        0.00000  1.315130e+05  1.461974e+09      0.000000      0.000000   \n",
      "25%    15394.50000  1.050567e+06  1.485821e+09      0.000000      0.000000   \n",
      "50%    30789.00000  1.835470e+06  1.506730e+09      0.000000      0.000000   \n",
      "75%    46183.50000  1.970801e+06  1.530317e+09      0.000000      0.000000   \n",
      "max    61578.00000  2.234805e+06  1.551312e+09      1.000000     47.000000   \n",
      "\n",
      "          N_CR_BRCH  N_CR_CHQ      N_CR_EFT   N_CR_MOBILE   N_CR_ONLINE  \\\n",
      "count  61579.000000   61579.0  61579.000000  61579.000000  61579.000000   \n",
      "mean       0.198542       0.0      0.910392      0.832881      0.381689   \n",
      "std        0.777817       0.0      1.989308      2.607528      1.411678   \n",
      "min        0.000000       0.0      0.000000      0.000000      0.000000   \n",
      "25%        0.000000       0.0      0.000000      0.000000      0.000000   \n",
      "50%        0.000000       0.0      0.000000      0.000000      0.000000   \n",
      "75%        0.000000       0.0      1.000000      0.000000      0.000000   \n",
      "max       20.000000       0.0     32.000000     67.000000     26.000000   \n",
      "\n",
      "           ...       NEW_FEATURE_91  NEW_FEATURE_92  NEW_FEATURE_93  \\\n",
      "count      ...         61579.000000    61579.000000    61579.000000   \n",
      "mean       ...           274.077501        5.345360      405.140381   \n",
      "std        ...           917.769052      179.244916      960.918621   \n",
      "min        ...             0.000000    -5500.000000        0.000000   \n",
      "25%        ...             0.000000        0.000000        0.000000   \n",
      "50%        ...             0.000000        0.000000        0.000000   \n",
      "75%        ...            88.957482        0.000000      221.685479   \n",
      "max        ...         44419.541667    29470.788286    22868.880000   \n",
      "\n",
      "       NEW_FEATURE_94  NEW_FEATURE_95  NEW_FEATURE_96  NEW_FEATURE_97  \\\n",
      "count    61579.000000         61579.0         61579.0         61579.0   \n",
      "mean        -1.469333             0.0             0.0             0.0   \n",
      "std        162.918700             0.0             0.0             0.0   \n",
      "min      -6255.889000             0.0             0.0             0.0   \n",
      "25%          0.000000             0.0             0.0             0.0   \n",
      "50%          0.000000             0.0             0.0             0.0   \n",
      "75%          0.000000             0.0             0.0             0.0   \n",
      "max      10921.800000             0.0             0.0             0.0   \n",
      "\n",
      "       NEW_FEATURE_98  NEW_FEATURE_99        STATUS  \n",
      "count         61579.0         61579.0  61579.000000  \n",
      "mean              0.0             0.0      0.035613  \n",
      "std               0.0             0.0      0.185324  \n",
      "min               0.0             0.0      0.000000  \n",
      "25%               0.0             0.0      0.000000  \n",
      "50%               0.0             0.0      0.000000  \n",
      "75%               0.0             0.0      0.000000  \n",
      "max               0.0             0.0      1.000000  \n",
      "\n",
      "[8 rows x 101 columns]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./learn')\n",
    "\n",
    "from learn_ann import get_learner_params_all\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "DIR = './organized_dataset/'\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "df = pd.read_csv(DIR+'feature_engineered_test.csv',header=0)\n",
    "df_35 = pd.read_csv(DIR+'sorted_35_test.csv',header=0)\n",
    "\n",
    "print('Data loading Done')\n",
    "print(df.describe())\n",
    "\n",
    "col_max_value = df.iloc[:,4:-1].max()\n",
    "col_min_value = df.iloc[:,4:-1].min()\n",
    "\n",
    "df.iloc[:,4:-1] = (df.iloc[:,4:-1] - col_min_value) / (col_max_value - col_min_value)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df_35.iloc[:,4:-1] = (df_35.iloc[:,4:-1] - col_min_value) / (col_max_value - col_min_value)\n",
    "df_35.fillna(0, inplace=True)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Long-term customers' records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term:  3\n",
      "# of short-term CID:  20\n",
      "dataset:  (61552, 101)\n",
      "# of long-term CID:  1982\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "SHORT_TERM = 3\n",
    "\n",
    "print('Term: ',SHORT_TERM)\n",
    "\n",
    "temp = df.groupby('CID').count()\n",
    "temp.rename(columns={temp.columns[0]:'length'}, inplace=True)\n",
    "\n",
    "# get as Series of CID with contract-length less than SHORT_TERM\n",
    "# temp = temp.loc[temp['length'] >= SHORT_TERM].iloc[:,0]\n",
    "temp = temp.loc[temp['length'] < SHORT_TERM].iloc[:,0]\n",
    "print('# of short-term CID: ',temp.shape[0])\n",
    "\n",
    "short_cid_set = set(temp.index.tolist())\n",
    "\n",
    "# remove short-term customers\n",
    "for cid in short_cid_set:\n",
    "    df.drop(df[df['CID']==cid].index, inplace=True)\n",
    "\n",
    "long_df = df\n",
    "print('dataset: ',long_df.shape)\n",
    "temp = long_df.groupby('CID').count()\n",
    "temp.rename(columns={temp.columns[0]:'length'}, inplace=True)\n",
    "temp = temp.loc[temp['length'] >= SHORT_TERM].iloc[:,0]\n",
    "print('# of long-term CID: ',temp.shape[0])\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Data\n",
    " Separate customers into group of customers with same length of records. Put each group into dictionary.\n",
    " So, Returning dict looks like: `dict = {1:group_1_month, 2:group_2_month, ..., 35:group_35_month}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing length: 3\n",
      "Processing length: 4\n",
      "Processing length: 5\n",
      "Processing length: 6\n",
      "Processing length: 7\n",
      "Processing length: 8\n",
      "Processing length: 9\n",
      "Processing length: 10\n",
      "Processing length: 11\n",
      "Processing length: 12\n",
      "Processing length: 13\n",
      "Processing length: 14\n",
      "Processing length: 15\n",
      "Processing length: 16\n",
      "Processing length: 17\n",
      "Processing length: 18\n",
      "Processing length: 19\n",
      "Processing length: 20\n",
      "Processing length: 21\n",
      "Processing length: 22\n",
      "Processing length: 23\n",
      "Processing length: 24\n",
      "Processing length: 25\n",
      "Processing length: 26\n",
      "Processing length: 27\n",
      "Processing length: 28\n",
      "Processing length: 29\n",
      "Processing length: 30\n",
      "Processing length: 31\n",
      "Processing length: 32\n",
      "Processing length: 33\n",
      "Processing length: 34\n",
      "For length: 35\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get df's dictionary = {length:df,...}\n",
    "def get_dict_dfs(df,short_term=6):\n",
    "    dict_of_df = {}\n",
    "  \n",
    "    length_df = df.groupby('CID').count()\n",
    "    length_df.rename(columns={length_df.columns[0]:'length'}, inplace=True)\n",
    "\n",
    "    # exclude 35-months customers.\n",
    "    for length in range(short_term,35):\n",
    "        print('Processing length:',length)\n",
    "        temp = length_df[length_df['length'] == length]\n",
    "        short_cid_list = set(temp.index.values)\n",
    "\n",
    "        current_df = pd.DataFrame(columns=df.columns)\n",
    "        for cid in short_cid_list:\n",
    "            temp = df.loc[df['CID']==cid].sort_values(by='DATE')\n",
    "            current_df = current_df.append(temp, ignore_index=True)\n",
    "\n",
    "        dict_of_df[length] = current_df\n",
    "    \n",
    "    # For 35-months customers\n",
    "    length = 35\n",
    "    print('For length:',length)\n",
    "    dict_of_df[length] = df_35\n",
    "\n",
    "    return dict_of_df\n",
    "\n",
    "dict_dfs = get_dict_dfs(df,short_term = SHORT_TERM)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (X,y) Batch Generator\n",
    " The dataset we have now is matrix. For LSTM to work, we must transform matrix into Tensor. (adding 3rd dimesion for time)\n",
    " Each tensor has `dimension = (batch_size, sequence_length, num_of_features_per_timestep)`. `sequence_length` means number of timesteps in other words.\n",
    " We set `sequence_length = 6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features:  97\n",
      "(61552, 101)\n",
      "X_train:  39900\n",
      "train batch:  1140\n",
      "X_test:  4410\n",
      "test batch:  126\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "NUM_OF_FEATURES = df.iloc[0,3:-1].count()\n",
    "# NUM_OF_FEATURE = 40\n",
    "INDEX_OF_LAST_FEATURE = NUM_OF_FEATURES + 3\n",
    "\n",
    "print('# of features: ',NUM_OF_FEATURES)\n",
    "\n",
    "#### get next training batch of customers with 35 months contract\n",
    "def next_train_batch_35(dict_dfs,num_of_features,index_of_last_feature,sequence_length,k=10,test_start_idx=0):\n",
    "    if test_start_idx<0 or test_start_idx>=k:\n",
    "        raise Exception(\"test_start_idx (%d) should be in range(0,k=%d)\"%(test_start_idx,k))\n",
    "    length = 35\n",
    "    \n",
    "    end = dict_dfs[length].shape[0]\n",
    "    temp = int(end*(test_start_idx/k))\n",
    "    test_start = temp - (temp % length)\n",
    "    temp = int(end*((test_start_idx+1)/k))\n",
    "    test_end = temp - (temp % length)\n",
    "    \n",
    "    df = dict_dfs[length].iloc[:test_start,:].append(dict_dfs[length].iloc[test_end:,:])\n",
    "    end = df.shape[0]\n",
    "    \n",
    "    temp_index_list = [list(range(i,i+sequence_length)) for i in range(0,end-sequence_length+1,length)]\n",
    "    batch_size = len(temp_index_list)\n",
    "    yield end, batch_size\n",
    "    \n",
    "    while True:\n",
    "        for start in range(0,length-sequence_length+1):\n",
    "            temp_index_list = [list(range(i,i+sequence_length)) for i in range(start,end-sequence_length+1,length)]\n",
    "            batch_size = len(temp_index_list)\n",
    "\n",
    "            X = np.zeros((batch_size,sequence_length,num_of_features))\n",
    "            y = np.zeros((batch_size,1))\n",
    "\n",
    "            for b in range(0,batch_size):\n",
    "                X[b,:,:] = df.iloc[temp_index_list[b],3:index_of_last_feature].values\n",
    "                y[b,0] = df.iloc[temp_index_list[b][-1],-1]\n",
    "\n",
    "            yield X,y.astype(int)\n",
    "\n",
    "#### get next test batch of customers with 35 months contract\n",
    "def next_test_batch_35(dict_dfs,num_of_features,index_of_last_feature,sequence_length,k=10,test_start_idx=0,):\n",
    "    if test_start_idx<0 or test_start_idx>=k:\n",
    "        raise Exception(\"test_start_idx (%d) should be in range(0,k=%d)\"%(test_start_idx,k))\n",
    "    \n",
    "    length = 35\n",
    "    end = dict_dfs[length].shape[0]\n",
    "    temp = int(end*(test_start_idx/k))\n",
    "    test_start = temp - (temp % length)\n",
    "    temp = int(end*((test_start_idx+1)/k))\n",
    "    test_end = temp - (temp % length)\n",
    "    \n",
    "    df = dict_dfs[length].iloc[test_start:test_end,:]\n",
    "    end = df.shape[0]\n",
    "    \n",
    "    temp_index_list = [list(range(i,i+sequence_length)) for i in range(0,end-sequence_length+1,length)]\n",
    "    batch_size = len(temp_index_list)\n",
    "    \n",
    "    yield end, batch_size\n",
    "    \n",
    "    while True:\n",
    "        for start in range(0,length-sequence_length+1):\n",
    "            temp_index_list = [list(range(i,i+sequence_length)) for i in range(start,end-sequence_length+1,length)]\n",
    "            batch_size = len(temp_index_list)\n",
    "\n",
    "            X = np.zeros((batch_size,sequence_length,num_of_features))\n",
    "            y = np.zeros((batch_size,1))\n",
    "\n",
    "            for b in range(0,batch_size):\n",
    "                X[b,:,:] = df.iloc[temp_index_list[b],3:index_of_last_feature].values\n",
    "                y[b,0] = df.iloc[temp_index_list[b][-1],-1]\n",
    "\n",
    "            yield X,y.astype(int)\n",
    "\n",
    "def next_train_batch(dict_dfs,num_of_features,index_of_last_feature,sequence_length,k=10,test_start_idx=0):\n",
    "    if test_start_idx<0 or test_start_idx>=k:\n",
    "        raise Exception(\"test_start_idx (%d) should be in range(0,k=%d)\"%(test_start_idx,k))\n",
    "    \n",
    "    while True:\n",
    "        for length in range(sequence_length,36):\n",
    "            end = dict_dfs[length].shape[0]\n",
    "            temp = int(end*(test_start_idx/k))\n",
    "            test_start = temp - (temp % length)\n",
    "            temp = int(end*((test_start_idx+1)/k))\n",
    "            test_end = temp - (temp % length)\n",
    "\n",
    "            df = dict_dfs[length].iloc[:test_start,:].append(dict_dfs[length].iloc[test_end:,:])\n",
    "            end = df.shape[0]\n",
    "            \n",
    "            MAX_BATCH_SIZE = 10000\n",
    "            for start in range(0,length-sequence_length+1):\n",
    "                temp_index_list = [list(range(i,i+sequence_length)) for i in range(start,end-sequence_length+1,length)]\n",
    "                batch_size = len(temp_index_list)\n",
    "\n",
    "                X = np.zeros((batch_size,sequence_length,num_of_features))\n",
    "                y = np.zeros((batch_size,1))\n",
    "            \n",
    "                for b in range(0,batch_size):\n",
    "                    X[b,:,:] = df.iloc[temp_index_list[b],3:index_of_last_feature].values\n",
    "                    y[b,0] = df.iloc[temp_index_list[b][-1],-1]\n",
    "\n",
    "                yield X,y.astype(int)\n",
    "\n",
    "def next_test_batch(dict_dfs,num_of_features,index_of_last_feature,sequence_length,k=10,test_start_idx=0):\n",
    "    if test_start_idx<0 or test_start_idx>=k:\n",
    "        raise Exception(\"test_start_idx (%d) should be in range(0,k=%d)\"%(test_start_idx,k))\n",
    "    \n",
    "    while True:\n",
    "        for length in range(sequence_length,36):\n",
    "            end = dict_dfs[length].shape[0]\n",
    "            temp = int(end*(test_start_idx/k))\n",
    "            test_start = temp - (temp % length)\n",
    "            temp = int(end*((test_start_idx+1)/k))\n",
    "            test_end = temp - (temp % length)\n",
    "\n",
    "            df = dict_dfs[length].iloc[test_start:test_end,:]\n",
    "            end = df.shape[0]\n",
    "            \n",
    "            for start in range(0,length-sequence_length+1):\n",
    "                temp_index_list = [list(range(i,i+sequence_length)) for i in range(start,end-sequence_length+1,length)]\n",
    "                batch_size = len(temp_index_list)\n",
    "\n",
    "                X = np.zeros((batch_size,sequence_length,num_of_features))\n",
    "                y = np.zeros((batch_size,1))\n",
    "            \n",
    "                for b in range(0,batch_size):\n",
    "                    X[b,:,:] = df.iloc[temp_index_list[b],3:index_of_last_feature].values\n",
    "                    y[b,0] = df.iloc[temp_index_list[b][-1],-1]\n",
    "\n",
    "                yield X,y.astype(int)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "SEQUENCE_LENGTH = SHORT_TERM\n",
    "\n",
    "k = 10\n",
    "\n",
    "train_batch_generator = next_train_batch_35(dict_dfs,\n",
    "                                            num_of_features = NUM_OF_FEATURES,\n",
    "                                            index_of_last_feature=INDEX_OF_LAST_FEATURE,\n",
    "                                            sequence_length=SEQUENCE_LENGTH)\n",
    "test_batch_generator = next_test_batch_35(dict_dfs,\n",
    "                                          num_of_features = NUM_OF_FEATURES,\n",
    "                                          index_of_last_feature=INDEX_OF_LAST_FEATURE,\n",
    "                                          sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "train_size, train_batch_size = next(train_batch_generator)\n",
    "test_size, test_batch_size = next(test_batch_generator)\n",
    "\n",
    "print(df.shape)\n",
    "print('X_train: ',train_size)\n",
    "print('train batch: ',train_batch_size)\n",
    "print('X_test: ',test_size)\n",
    "print('test batch: ',test_batch_size)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1140, 3, 97)\n",
      "(1140, 1)\n"
     ]
    }
   ],
   "source": [
    "X,y = next(train_batch_generator)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# X,y = next(train_batch_generator)\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn LSTM\n",
    " We use AUC and k-folds cross-validation for evaluating model.\n",
    " Parameters are not tuned. So, it still has room for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning LSTM\n",
      "# of features:  97\n",
      "Sequence length:  3\n",
      "================ 1/5 th Learning ================\n",
      "(61552, 101)\n",
      "X_train:  35455\n",
      "train batch:  1013\n",
      "X_test:  8855\n",
      "test batch:  253\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 82s 824ms/step - loss: 0.7597 - auc2: 0.8646\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 82s 823ms/step - loss: 0.5354 - auc2: 0.9275\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 83s 833ms/step - loss: 0.5042 - auc2: 0.9379\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 83s 831ms/step - loss: 0.4841 - auc2: 0.9439\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 82s 819ms/step - loss: 0.4668 - auc2: 0.9479\n",
      "Epoch 6/10\n",
      "  1/100 [..............................] - ETA: 1:40 - loss: 0.3473 - auc2: 0.9494WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.117944). Check your callbacks.\n",
      "100/100 [==============================] - 82s 824ms/step - loss: 0.4493 - auc2: 0.9508\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 83s 833ms/step - loss: 0.4365 - auc2: 0.9531\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 81s 814ms/step - loss: 0.4260 - auc2: 0.9550\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 82s 815ms/step - loss: 0.4174 - auc2: 0.9566\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 84s 835ms/step - loss: 0.4032 - auc2: 0.9579\n",
      "1/5 th TEST AUC: 0.95574\n",
      "================ 2/5 th Learning ================\n",
      "(61552, 101)\n",
      "X_train:  35455\n",
      "train batch:  1013\n",
      "X_test:  8855\n",
      "test batch:  253\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 82s 820ms/step - loss: 1.0284 - auc2: 0.8605\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 82s 824ms/step - loss: 0.6460 - auc2: 0.9125\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 81s 807ms/step - loss: 0.5910 - auc2: 0.9243\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 82s 823ms/step - loss: 0.5592 - auc2: 0.9316\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 82s 815ms/step - loss: 0.5383 - auc2: 0.9368\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 82s 818ms/step - loss: 0.5208 - auc2: 0.9408\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 80s 801ms/step - loss: 0.5086 - auc2: 0.9438\n",
      "Epoch 8/10\n",
      "  1/100 [..............................] - ETA: 1:23 - loss: 0.5430 - auc2: 0.9452WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.307385). Check your callbacks.\n",
      "  2/100 [..............................] - ETA: 1:35 - loss: 0.5072 - auc2: 0.9452WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.211426). Check your callbacks.\n",
      "  3/100 [..............................] - ETA: 1:31 - loss: 0.4307 - auc2: 0.9452WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.115468). Check your callbacks.\n",
      "  4/100 [>.............................] - ETA: 1:30 - loss: 0.4421 - auc2: 0.9453WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.107785). Check your callbacks.\n",
      "100/100 [==============================] - 82s 823ms/step - loss: 0.5022 - auc2: 0.9463\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 81s 812ms/step - loss: 0.4887 - auc2: 0.9484\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 81s 811ms/step - loss: 0.4751 - auc2: 0.9501\n",
      "2/5 th TEST AUC: 0.95084\n",
      "================ 3/5 th Learning ================\n",
      "(61552, 101)\n",
      "X_train:  35455\n",
      "train batch:  1013\n",
      "X_test:  8855\n",
      "test batch:  253\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 82s 822ms/step - loss: 0.7500 - auc2: 0.9036\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 81s 813ms/step - loss: 0.5542 - auc2: 0.9291\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 82s 816ms/step - loss: 0.5126 - auc2: 0.9383\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 81s 807ms/step - loss: 0.4932 - auc2: 0.9441\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 81s 814ms/step - loss: 0.4759 - auc2: 0.9482\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 81s 809ms/step - loss: 0.4522 - auc2: 0.9514\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 82s 822ms/step - loss: 0.4341 - auc2: 0.9540\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 81s 809ms/step - loss: 0.4210 - auc2: 0.9563\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 82s 823ms/step - loss: 0.4126 - auc2: 0.9583\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 82s 819ms/step - loss: 0.4042 - auc2: 0.9599\n",
      "3/5 th TEST AUC: 0.95791\n",
      "================ 4/5 th Learning ================\n",
      "(61552, 101)\n",
      "X_train:  35455\n",
      "train batch:  1013\n",
      "X_test:  8855\n",
      "test batch:  253\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 82s 821ms/step - loss: 0.7972 - auc2: 0.8978\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 81s 808ms/step - loss: 0.5703 - auc2: 0.9298\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 83s 832ms/step - loss: 0.5291 - auc2: 0.9397\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 80s 803ms/step - loss: 0.5020 - auc2: 0.9455\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 81s 807ms/step - loss: 0.4838 - auc2: 0.9494\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 84s 841ms/step - loss: 0.4641 - auc2: 0.9524\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 81s 812ms/step - loss: 0.4553 - auc2: 0.9547\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 81s 813ms/step - loss: 0.4455 - auc2: 0.9565\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 82s 820ms/step - loss: 0.4304 - auc2: 0.9582\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 84s 837ms/step - loss: 0.4197 - auc2: 0.9596\n",
      "4/5 th TEST AUC: 0.95974\n",
      "================ 5/5 th Learning ================\n",
      "(61552, 101)\n",
      "X_train:  35420\n",
      "train batch:  1012\n",
      "X_test:  8890\n",
      "test batch:  254\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 81s 812ms/step - loss: 0.9413 - auc2: 0.8795\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 82s 821ms/step - loss: 0.6155 - auc2: 0.9119\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 82s 817ms/step - loss: 0.5767 - auc2: 0.9226\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 83s 826ms/step - loss: 0.5510 - auc2: 0.9291\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 82s 816ms/step - loss: 0.5303 - auc2: 0.9341\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 83s 831ms/step - loss: 0.5146 - auc2: 0.9380\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 85s 853ms/step - loss: 0.5005 - auc2: 0.9412\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 85s 853ms/step - loss: 0.4889 - auc2: 0.9439\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 84s 837ms/step - loss: 0.4766 - auc2: 0.9462\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 83s 829ms/step - loss: 0.4612 - auc2: 0.9484\n",
      "5/5 th TEST AUC: 0.94665\n",
      "================ Result ================\n",
      "OVERALL TEST AUC: 0.95418 (+/- 0.0048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "K = tf.keras.backend\n",
    "\n",
    "# https://www.kaggle.com/c/invasive-species-monitoring/discussion/32762\n",
    "# calculate AUC\n",
    "def auc2(y_true, y_pred):\n",
    "    # https://stackoverflow.com/questions/48174323/tensorflow-1-4-tf-metrics-auc-for-auc-calculation\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "print('Learning LSTM')\n",
    "print('# of features: ',NUM_OF_FEATURES)  \n",
    "print('Sequence length: ',SEQUENCE_LENGTH)\n",
    "\n",
    "cv_list = []     ## store cross-validation scores ##\n",
    "num_of_folds = 5 ## k of k-fold cv ##\n",
    "for k_index in range(num_of_folds):\n",
    "    print('================ %d/%d th Learning ================'%(k_index+1,num_of_folds))\n",
    "    train_batch_generator = next_train_batch_35(dict_dfs,\n",
    "                                            num_of_features = NUM_OF_FEATURES,\n",
    "                                            index_of_last_feature=INDEX_OF_LAST_FEATURE,\n",
    "                                            sequence_length=SEQUENCE_LENGTH,\n",
    "                                            k=num_of_folds,\n",
    "                                            test_start_idx=k_index)\n",
    "    test_batch_generator = next_test_batch_35(dict_dfs,\n",
    "                                            num_of_features = NUM_OF_FEATURES,\n",
    "                                            index_of_last_feature=INDEX_OF_LAST_FEATURE,\n",
    "                                            sequence_length=SEQUENCE_LENGTH,\n",
    "                                            k=num_of_folds,\n",
    "                                            test_start_idx=k_index)\n",
    "\n",
    "    train_size, train_batch_size = next(train_batch_generator)\n",
    "    test_size, test_batch_size = next(test_batch_generator)\n",
    "\n",
    "    print(df.shape)\n",
    "    print('X_train: ',train_size)\n",
    "    print('train batch: ',train_batch_size)\n",
    "    print('X_test: ',test_size)\n",
    "    print('test batch: ',test_batch_size)\n",
    "  \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.LSTM(50, batch_input_shape=(None, SEQUENCE_LENGTH, NUM_OF_FEATURES),stateful=False,activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(32,kernel_initializer='normal', activation='relu'))\n",
    "    model.add(keras.layers.Dense(1,kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[auc2])\n",
    "\n",
    "    #### fit model with next train batch ####\n",
    "    model.fit_generator(generator=train_batch_generator, \n",
    "                        steps_per_epoch=100, epochs=10, verbose=1, \n",
    "                        class_weight = {0: 1,1: 33})\n",
    "    \n",
    "    #### evaluate model by next test batch ####\n",
    "    result = model.evaluate_generator(generator=test_batch_generator, steps=int(test_size/test_batch_size))\n",
    "    print('%d/%d th TEST AUC: %.5f'%(k_index+1,num_of_folds,result[1]))\n",
    "    cv_list.append(result[1])\n",
    "\n",
    "print('================ Result ================')\n",
    "print('OVERALL TEST AUC: %.5f (+/- %.4f)'%(np.mean(cv_list),np.std(cv_list)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
